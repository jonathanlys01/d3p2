"""
Minimalist LLaDA diffusion sampler, adapted from the LLaDA codebase, draft generated by Gemini
"""

from typing import Optional

import torch
import torch.nn.functional as F
from torch import nn
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer

from config import Config


torch.set_float32_matmul_precision("high")


def add_gumbel_noise(logits, temperature):
    """
    The Gumbel max is a method for sampling categorical distributions.
    Reference: src/llada/generate.py
    """
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    # Generate Gumbel noise
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = -torch.log(-torch.log(noise))
    # Apply noise: gumbel_noise is scaled by temperature, but in the reference
    # implementation (generate.py, add_gumbel_noise), the temperature is
    # applied to the exponent of the noise, not as a scaling factor.
    # We follow the reference: logits.exp() / ((-torch.log(noise)) ** temperature)
    # Wait, the reference is: gumbel_noise = (-torch.log(noise)) ** temperature
    # return logits.exp() / gumbel_noise
    # This seems numerically unstable.

    # Let's re-read the reference `add_gumbel_noise`
    # gumbel_noise = (-torch.log(noise)) ** temperature
    # return logits.exp() / gumbel_noise

    # Ah, the *other* Gumbel trick is logits + gumbel_noise.
    # Gumbel-Max trick: argmax(logits + G) ~ sample(softmax(logits))
    # Gumbel-Softmax uses (logits + G) / temp

    # The reference code `src/llada/generate.py` does this:
    noise = torch.rand_like(logits, dtype=torch.float64)
    # This is not standard Gumbel noise, but what's in the file.
    gumbel_noise = (-torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise
    # This seems to be a custom sampling method.
    # Let's stick to the reference implementation exactly.

    # Re-checking src/llada/generate.py
    # ...
    # logits = logits.to(torch.float64)
    # noise = torch.rand_like(logits, dtype=torch.float64)
    # gumbel_noise = (-torch.log(noise)) ** temperature
    # return logits.exp() / gumbel_noise
    #
    # This is... unusual. `temperature` is an exponent on the noise.
    # A standard Gumbel noise addition would be:
    # noise = torch.rand_like(logits)
    # gumbel_noise = -torch.log(-torch.log(noise + 1e-9) + 1e-9)
    # return (logits + gumbel_noise) / temperature


def get_num_transfer_tokens(mask_index, steps):
    """
    Precompute the number of tokens to transition at each step.
    Employs a linear noise schedule.
    Reference: src/llada/generate.py
    """
    mask_num = mask_index.sum(dim=1, keepdim=True)

    base = mask_num // steps
    remainder = mask_num % steps

    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base

    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, : remainder[i]] += 1

    return num_transfer_tokens


class LLaDASampler(nn.Module):
    """
    Minimalist Discrete Diffusion Sampler for LLaDA.

    Follows the structure of MDLMSampler and the logic of llada/generate.py.
    """

    def __init__(self, config: Config):
        super().__init__()
        self.config = config

        # Load LLaDA model and tokenizer
        # Assumes config has `llada_model_path` and `cache_dir`
        self.model = AutoModel.from_pretrained(
            config.llada_model_path,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,  # From llada/generate.py main
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.llada_model_path,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
        )

        # LLaDA uses a specific mask token ID
        # Reference: src/llada/generate.py
        self.mask_id = 126336

        # Assumes config has `gen_length`
        self.model_length = config.gen_length

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()

    def _sample_prior(self, *batch_dims) -> torch.Tensor:
        """
        Creates an all-mask tensor, similar to MDLMSampler.
        """
        return self.mask_id * torch.ones(*batch_dims, dtype=torch.int64)

    @torch.no_grad()
    def _llada_step(
        self,
        x: torch.Tensor,
        i: int,
        num_transfer_tokens_schedule: torch.Tensor,
        temperature: float,
    ) -> torch.Tensor:
        """
        Performs a single reverse diffusion step based on llada/generate.py.
        """
        # 1. Identify which tokens are currently masked
        current_mask = x == self.mask_id

        # 2. Get model predictions (logits) for the full sequence
        # We don't use CFG for this minimalist implementation
        logits = self.model(x).logits

        # 3. Apply Gumbel noise and get x0 prediction (the "denoised" sequence)
        logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
        x0_pred = torch.argmax(logits_with_noise, dim=-1)  # (B, L)

        # 4. Get confidence scores for the x0 prediction
        # Using "low_confidence" remasking strategy from generate.py
        p = F.softmax(logits, dim=-1)
        x0_p = torch.gather(p, -1, x0_pred.unsqueeze(-1)).squeeze(-1)  # (B, L)

        # 5. We only care about the confidence of *currently masked* tokens
        confidence = torch.where(current_mask, x0_p, -torch.inf)

        # 6. Find the k highest-confidence tokens to unmask this step
        x_new = x.clone()
        for j in range(confidence.shape[0]):  # Iterate over batch
            # Get k for this batch item and step
            k = num_transfer_tokens_schedule[j, i].item()
            if k == 0:
                continue

            # Find the top-k most confident predictions
            _, select_index = torch.topk(confidence[j], k=k)

            # 7. Update the sequence: copy the predicted tokens into the sequence
            x_new[j, select_index] = x0_pred[j, select_index]

        return x_new

    @torch.no_grad()
    def sample(
        self,
        num_steps: Optional[int] = None,
        gen_length: Optional[int] = None,
        batch_size: Optional[int] = None,
        temperature: Optional[float] = None,
    ):
        """
        Generates text from a full mask, following the llada/generate.py logic.
        """
        # Get generation parameters from config if not provided
        num_steps = num_steps or self.config.num_steps
        gen_length = gen_length or self.config.gen_length
        batch_size = batch_size or self.config.batch_size
        temperature = temperature if temperature is not None else self.config.temperature

        # 1. Start with a fully masked sequence
        x = self._sample_prior(batch_size, gen_length).to(self.device)

        # 2. Pre-calculate the linear schedule of tokens to unmask
        # We calculate this once based on the initial full mask
        initial_mask = x == self.mask_id
        num_transfer_tokens = get_num_transfer_tokens(initial_mask, num_steps)

        # 3. Run the reverse diffusion process
        disable_tqdm = False  # Add any distributed rank checks here if needed
        for i in tqdm(range(num_steps), desc="Generating (LLaDA)", disable=disable_tqdm):
            x = self._llada_step(
                x=x,
                i=i,
                num_transfer_tokens_schedule=num_transfer_tokens,
                temperature=temperature,
            )

        return x
